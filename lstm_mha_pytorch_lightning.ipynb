{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "lstm-mha-pytorch-lightning.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ambideXtrous9/PyTorch-Models/blob/main/lstm_mha_pytorch_lightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re  # For preprocessing\n",
        "import pandas as pd  # For data handling\n",
        "from time import time  # To time our operations\n",
        "from collections import defaultdict  # For word frequency\n",
        "# Tools for preprocessing input data\n",
        "from bs4 import BeautifulSoup\n",
        "from wordcloud import WordCloud\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot as plt\n",
        "# Tools for creating ngrams and vectorizing input data\n",
        "from gensim.models import Word2Vec, Phrases\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import warnings\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "import torchmetrics\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning import seed_everything\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from pytorch_lightning import Trainer\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import gensim\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "%config Completer.use_jedi = False\n",
        "import spacy  "
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-05-22T10:15:24.136526Z",
          "iopub.execute_input": "2022-05-22T10:15:24.136909Z",
          "iopub.status.idle": "2022-05-22T10:15:38.866030Z",
          "shell.execute_reply.started": "2022-05-22T10:15:24.136805Z",
          "shell.execute_reply": "2022-05-22T10:15:38.865251Z"
        },
        "trusted": true,
        "id": "cC0DFG6IWPHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:15:38.868264Z",
          "iopub.execute_input": "2022-05-22T10:15:38.868590Z",
          "iopub.status.idle": "2022-05-22T10:15:49.558854Z",
          "shell.execute_reply.started": "2022-05-22T10:15:38.868546Z",
          "shell.execute_reply": "2022-05-22T10:15:49.557923Z"
        },
        "trusted": true,
        "id": "IGOjrUbOWPHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('../input/bias-of-us-news-media-houses/Train.xlsx',nrows=15000)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:15:49.560908Z",
          "iopub.execute_input": "2022-05-22T10:15:49.561228Z",
          "iopub.status.idle": "2022-05-22T10:16:04.907302Z",
          "shell.execute_reply.started": "2022-05-22T10:15:49.561172Z",
          "shell.execute_reply": "2022-05-22T10:16:04.906348Z"
        },
        "trusted": true,
        "id": "teVvwl6vWPHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:16:04.909471Z",
          "iopub.execute_input": "2022-05-22T10:16:04.909746Z",
          "iopub.status.idle": "2022-05-22T10:16:04.943969Z",
          "shell.execute_reply.started": "2022-05-22T10:16:04.909709Z",
          "shell.execute_reply": "2022-05-22T10:16:04.943040Z"
        },
        "trusted": true,
        "id": "HAE6Iz27WPHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:16:04.945588Z",
          "iopub.execute_input": "2022-05-22T10:16:04.945881Z",
          "iopub.status.idle": "2022-05-22T10:16:04.972819Z",
          "shell.execute_reply.started": "2022-05-22T10:16:04.945842Z",
          "shell.execute_reply": "2022-05-22T10:16:04.971875Z"
        },
        "trusted": true,
        "id": "wOGKLYjiWPHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing - Punctuations,Stopwords"
      ],
      "metadata": {
        "id": "l6KS_nxzWPHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "def stemm_text(text):\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    return ' '.join([stemmer.stem(w) for w in text.split(' ')])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:16:04.974739Z",
          "iopub.execute_input": "2022-05-22T10:16:04.975467Z",
          "iopub.status.idle": "2022-05-22T10:16:04.980801Z",
          "shell.execute_reply.started": "2022-05-22T10:16:04.975417Z",
          "shell.execute_reply": "2022-05-22T10:16:04.980140Z"
        },
        "trusted": true,
        "id": "RRprU8qcWPHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ' '.join([lemmatizer.lemmatize(w) for w in text.split(' ')])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:16:04.982658Z",
          "iopub.execute_input": "2022-05-22T10:16:04.983233Z",
          "iopub.status.idle": "2022-05-22T10:16:04.991299Z",
          "shell.execute_reply.started": "2022-05-22T10:16:04.983178Z",
          "shell.execute_reply": "2022-05-22T10:16:04.990549Z"
        },
        "trusted": true,
        "id": "4LNVbPdxWPHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe - 100d"
      ],
      "metadata": {
        "id": "6KkbSwI2WPHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Sentence2Vec(T,embedding_dim = 100,max_length = 120):\n",
        "    T = T.str.split(' \\n\\n---\\n\\n').str[0]\n",
        "    T = T.str.replace('-',' ').str.replace('[^\\w\\s]','').str.replace('\\n',' ').str.lower()\n",
        "    stop = stopwords.words('english')\n",
        "    T = T.apply(lambda x: ' '.join(x for x in x.split() if  not x.isdigit()))\n",
        "    T = T.apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n",
        "    #T = T.apply(stemm_text)\n",
        "    #T = T.apply(lemmatize_text)\n",
        "    glove_path = '../input/glove6b100dtxt/glove.6B.100d.txt'\n",
        "    path = glove_path\n",
        "    tokenizer = Tokenizer()\n",
        "    text=T\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    word_index=tokenizer.word_index\n",
        "    print(\"number of word in vocabulary\",len(word_index))\n",
        "    vocab_size = 5000\n",
        "    trunc_type = 'post'\n",
        "    oov_tok = '<OOV>'\n",
        "    padding_type = 'post'\n",
        "    #print(\"words in vocab\",word_index)\n",
        "    text_sequence=tokenizer.texts_to_sequences(text)\n",
        "    text_sequence = pad_sequences(text_sequence, maxlen=max_length, truncating=trunc_type,padding=padding_type)\n",
        "    print(\"word in sentences are replaced with word ID\",text_sequence)\n",
        "    size_of_vocabulary=len(tokenizer.word_index) + 1\n",
        "    print(\"The size of vocabulary \",size_of_vocabulary)\n",
        "    embeddings_index = dict()\n",
        "\n",
        "    f = open(path)\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "    f.close()\n",
        "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "    embedding_matrix = np.zeros((size_of_vocabulary, embedding_dim))\n",
        "\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    text_shape = text_sequence.shape\n",
        "    X_train = np.empty((text_shape[0],text_shape[1],embedding_matrix.shape[1]))\n",
        "    for i in range(text_sequence.shape[0]):\n",
        "        for j in range(text_sequence.shape[1]):\n",
        "            X_train[i,j,:] = embedding_matrix[text_sequence[i][j]]\n",
        "    print(X_train.shape)\n",
        "\n",
        "    return X_train,embeddings_index,word_index\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:16:04.993024Z",
          "iopub.execute_input": "2022-05-22T10:16:04.993398Z",
          "iopub.status.idle": "2022-05-22T10:16:05.010005Z",
          "shell.execute_reply.started": "2022-05-22T10:16:04.993358Z",
          "shell.execute_reply": "2022-05-22T10:16:05.009096Z"
        },
        "trusted": true,
        "id": "-QJvyqZxWPHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_body,embeddings_index,word_index = Sentence2Vec(df['content_original'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:16:05.011483Z",
          "iopub.execute_input": "2022-05-22T10:16:05.012424Z",
          "iopub.status.idle": "2022-05-22T10:17:01.589248Z",
          "shell.execute_reply.started": "2022-05-22T10:16:05.012391Z",
          "shell.execute_reply": "2022-05-22T10:17:01.588252Z"
        },
        "trusted": true,
        "id": "QBxLjVU-WPHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_head,embeddings_index2,word_index2 = Sentence2Vec(df['title'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:01.593248Z",
          "iopub.execute_input": "2022-05-22T10:17:01.593458Z",
          "iopub.status.idle": "2022-05-22T10:17:16.163631Z",
          "shell.execute_reply.started": "2022-05-22T10:17:01.593431Z",
          "shell.execute_reply": "2022-05-22T10:17:16.162783Z"
        },
        "trusted": true,
        "id": "POLvBclAWPHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA Projection of Vocab"
      ],
      "metadata": {
        "id": "DSGWEiTNWPHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_matrix = np.zeros((len(word_index)+1, 100))\n",
        "\n",
        "for word, index in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    emb_matrix[index, :] = embedding_vector"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:16.164946Z",
          "iopub.execute_input": "2022-05-22T10:17:16.165772Z",
          "iopub.status.idle": "2022-05-22T10:17:16.343431Z",
          "shell.execute_reply.started": "2022-05-22T10:17:16.165728Z",
          "shell.execute_reply": "2022-05-22T10:17:16.342591Z"
        },
        "trusted": true,
        "id": "fKRfgejrWPHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:16.344742Z",
          "iopub.execute_input": "2022-05-22T10:17:16.345016Z",
          "iopub.status.idle": "2022-05-22T10:17:16.406371Z",
          "shell.execute_reply.started": "2022-05-22T10:17:16.344980Z",
          "shell.execute_reply": "2022-05-22T10:17:16.405673Z"
        },
        "trusted": true,
        "id": "PghOks0yWPHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "\n",
        "z = pca.fit_transform(emb_matrix)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:16.407819Z",
          "iopub.execute_input": "2022-05-22T10:17:16.408068Z",
          "iopub.status.idle": "2022-05-22T10:17:17.636754Z",
          "shell.execute_reply.started": "2022-05-22T10:17:16.408033Z",
          "shell.execute_reply": "2022-05-22T10:17:17.635893Z"
        },
        "trusted": true,
        "id": "tuW3HzJdWPHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = pd.DataFrame()\n",
        "df3[\"comp-1\"] = z[:,0]\n",
        "df3[\"comp-2\"] = z[:,1]\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.scatterplot(x=\"comp-1\", y=\"comp-2\",data=df3).set(title=\"Glove PCA projection\") "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:17.638178Z",
          "iopub.execute_input": "2022-05-22T10:17:17.638466Z",
          "iopub.status.idle": "2022-05-22T10:17:18.092686Z",
          "shell.execute_reply.started": "2022-05-22T10:17:17.638428Z",
          "shell.execute_reply": "2022-05-22T10:17:18.088994Z"
        },
        "trusted": true,
        "id": "srmHXvydWPHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA Projection based on Sentiment"
      ],
      "metadata": {
        "id": "-18xjA8SWPHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.sum(X_train_body,axis=1)\n",
        "print(X.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:18.093733Z",
          "iopub.execute_input": "2022-05-22T10:17:18.093977Z",
          "iopub.status.idle": "2022-05-22T10:17:18.279809Z",
          "shell.execute_reply.started": "2022-05-22T10:17:18.093940Z",
          "shell.execute_reply": "2022-05-22T10:17:18.278894Z"
        },
        "trusted": true,
        "id": "o1FiTs7NWPHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "\n",
        "z = pca.fit_transform(X)\n",
        "df3 = pd.DataFrame()\n",
        "df3[\"comp-1\"] = z[:,0]\n",
        "df3[\"comp-2\"] = z[:,1]\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.scatterplot(x=\"comp-1\", y=\"comp-2\",hue=df['bias'].tolist(),palette=sns.color_palette(\"hls\", 3),data=df3).set(title=\"Glove PCA projection\") "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:18.281271Z",
          "iopub.execute_input": "2022-05-22T10:17:18.281489Z",
          "iopub.status.idle": "2022-05-22T10:17:19.215609Z",
          "shell.execute_reply.started": "2022-05-22T10:17:18.281460Z",
          "shell.execute_reply": "2022-05-22T10:17:19.212949Z"
        },
        "trusted": true,
        "id": "NZpdad-lWPHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:19.217021Z",
          "iopub.execute_input": "2022-05-22T10:17:19.217968Z",
          "iopub.status.idle": "2022-05-22T10:17:19.240700Z",
          "shell.execute_reply.started": "2022-05-22T10:17:19.217927Z",
          "shell.execute_reply": "2022-05-22T10:17:19.239840Z"
        },
        "trusted": true,
        "id": "02PqDUjuWPHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style = \"darkgrid\" , font_scale = 1.2)\n",
        "sns.countplot(df.bias)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:19.242129Z",
          "iopub.execute_input": "2022-05-22T10:17:19.242492Z",
          "iopub.status.idle": "2022-05-22T10:17:19.415121Z",
          "shell.execute_reply.started": "2022-05-22T10:17:19.242451Z",
          "shell.execute_reply": "2022-05-22T10:17:19.414256Z"
        },
        "trusted": true,
        "id": "UMTwlDKzWPHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Cloud"
      ],
      "metadata": {
        "id": "DsjHD9YxWPHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:19.419411Z",
          "iopub.execute_input": "2022-05-22T10:17:19.419776Z",
          "iopub.status.idle": "2022-05-22T10:17:19.428677Z",
          "shell.execute_reply.started": "2022-05-22T10:17:19.419731Z",
          "shell.execute_reply": "2022-05-22T10:17:19.427650Z"
        },
        "trusted": true,
        "id": "-8wfLGFZWPHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df['bias'].to_numpy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:19.430592Z",
          "iopub.execute_input": "2022-05-22T10:17:19.430912Z",
          "iopub.status.idle": "2022-05-22T10:17:19.438216Z",
          "shell.execute_reply.started": "2022-05-22T10:17:19.430871Z",
          "shell.execute_reply": "2022-05-22T10:17:19.437405Z"
        },
        "trusted": true,
        "id": "zdML80BDWPHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:19.439712Z",
          "iopub.execute_input": "2022-05-22T10:17:19.440079Z",
          "iopub.status.idle": "2022-05-22T10:17:19.450121Z",
          "shell.execute_reply.started": "2022-05-22T10:17:19.440035Z",
          "shell.execute_reply": "2022-05-22T10:17:19.449338Z"
        },
        "trusted": true,
        "id": "SwLxS5gmWPHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Test Split"
      ],
      "metadata": {
        "id": "q_iYTNrcWPHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(X_train_body, Y,test_size=0.2,random_state=42)\n",
        "X_train_h, X_val_h, y_train_h, y_val_h = train_test_split(X_train_head, Y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:19.451611Z",
          "iopub.execute_input": "2022-05-22T10:17:19.451813Z",
          "iopub.status.idle": "2022-05-22T10:17:20.269753Z",
          "shell.execute_reply.started": "2022-05-22T10:17:19.451787Z",
          "shell.execute_reply": "2022-05-22T10:17:20.268952Z"
        },
        "trusted": true,
        "id": "_OP2iiKzWPHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_b.shape)\n",
        "print(X_train_h.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:20.271057Z",
          "iopub.execute_input": "2022-05-22T10:17:20.271328Z",
          "iopub.status.idle": "2022-05-22T10:17:20.279125Z",
          "shell.execute_reply.started": "2022-05-22T10:17:20.271291Z",
          "shell.execute_reply": "2022-05-22T10:17:20.277957Z"
        },
        "trusted": true,
        "id": "3Nx24G9tWPHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Torch Tensor and Dataloader"
      ],
      "metadata": {
        "id": "1PinFa37WPHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_b = torch.Tensor(X_train_b)\n",
        "train_targets_b = torch.Tensor(y_train_b).type(torch.LongTensor)\n",
        "val_features_b = torch.Tensor(X_val_b)\n",
        "val_targets_b = torch.Tensor(y_val_b).type(torch.LongTensor)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:20.280714Z",
          "iopub.execute_input": "2022-05-22T10:17:20.281121Z",
          "iopub.status.idle": "2022-05-22T10:17:20.812390Z",
          "shell.execute_reply.started": "2022-05-22T10:17:20.281082Z",
          "shell.execute_reply": "2022-05-22T10:17:20.811583Z"
        },
        "trusted": true,
        "id": "F3z5V1aNWPHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_h = torch.Tensor(X_train_h)\n",
        "train_targets_h = torch.Tensor(y_train_h).type(torch.LongTensor)\n",
        "val_features_h = torch.Tensor(X_val_h)\n",
        "val_targets_h = torch.Tensor(y_val_h).type(torch.LongTensor)\n",
        "\n",
        "\n",
        "train = TensorDataset(train_features_b,train_features_h, train_targets_h)\n",
        "val = TensorDataset(val_features_b,val_features_h, val_targets_h)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:20.813911Z",
          "iopub.execute_input": "2022-05-22T10:17:20.814208Z",
          "iopub.status.idle": "2022-05-22T10:17:21.361474Z",
          "shell.execute_reply.started": "2022-05-22T10:17:20.814156Z",
          "shell.execute_reply": "2022-05-22T10:17:21.359870Z"
        },
        "trusted": true,
        "id": "rha557qSWPHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BS = 16"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:17:21.367030Z",
          "iopub.execute_input": "2022-05-22T10:17:21.367771Z",
          "iopub.status.idle": "2022-05-22T10:17:21.375386Z",
          "shell.execute_reply.started": "2022-05-22T10:17:21.367725Z",
          "shell.execute_reply": "2022-05-22T10:17:21.374355Z"
        },
        "trusted": true,
        "id": "e9xV5wFsWPHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Architecture"
      ],
      "metadata": {
        "id": "aRtlHIkZWPHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LitLSTM(pl.LightningModule):\n",
        "    def __init__(self,num_classes,dimension,hidd_dim):\n",
        "        super(LitLSTM, self).__init__()\n",
        "        \n",
        "        self.hidd_dim = hidd_dim\n",
        "        \n",
        "        self.LSTM = nn.LSTM(input_size=dimension,\n",
        "                            hidden_size=self.hidd_dim,\n",
        "                            num_layers=1,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "        \n",
        "        self.mha = nn.MultiheadAttention(self.hidd_dim,2,dropout=0.5,batch_first=True,kdim=self.hidd_dim,vdim=self.hidd_dim)\n",
        "        self.model = nn.Sequential(\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Linear(self.hidd_dim, num_classes),\n",
        "                                    nn.LogSoftmax(dim=1))\n",
        "        \n",
        "        # add metrics\n",
        "        self.train_acc = torchmetrics.Accuracy()\n",
        "        self.train_f1 = torchmetrics.F1(average=\"micro\")\n",
        "        self.val_acc = torchmetrics.Accuracy()\n",
        "        self.val_f1 = torchmetrics.F1(average=\"micro\")\n",
        "        \n",
        "    def forward(self, body,head):\n",
        "        \n",
        "        head, (final_hidden_state_head, final_cell_state_head) = self.LSTM(head)\n",
        "        body, (final_hidden_state_body, final_cell_state_body) = self.LSTM(body)\n",
        "        \n",
        "        h = final_hidden_state_head[-1].reshape(BS,1,-1)\n",
        "        b = final_hidden_state_body[-1].reshape(BS,1,-1)\n",
        "        \n",
        "        mha, mha_wgts = self.mha(h,h,b)\n",
        "        out = self.model(mha).reshape(-1,3)\n",
        "        \n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        data1,data2,labels = batch\n",
        "        # Forward pass\n",
        "       \n",
        "        outputs = self.forward(data1,data2)\n",
        "        lossfn = nn.NLLLoss()\n",
        "        loss = lossfn(outputs, labels)\n",
        "        \n",
        "        y_pred = torch.exp(outputs)\n",
        "        #y_pred = output.data.max(1, keepdim=True)[1]\n",
        "        acc = self.train_acc(y_pred, labels)\n",
        "        f1 = self.train_f1(y_pred, labels)\n",
        "        # just accumulate\n",
        "\n",
        "        self.log(\"train_loss\", loss)\n",
        "        self.log(\"train_accuracy\", acc)\n",
        "        self.log(\"train_f1\", f1)\n",
        "        tensorboard_logs = {'train_loss': loss}\n",
        "        # use key 'log'\n",
        "        return {\"loss\": loss, 'log': tensorboard_logs}\n",
        "\n",
        "    # define what happens for testing here\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        trainD = DataLoader(train, num_workers=4,batch_size=BS, shuffle=True,drop_last=True)\n",
        "        \n",
        "\n",
        "        return trainD\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        valD = DataLoader(val, num_workers=4,batch_size=BS, shuffle=True,drop_last=True)\n",
        "        \n",
        "\n",
        "        return valD\n",
        "    \n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        data1,data2,labels = batch\n",
        "        # Forward pass\n",
        "       \n",
        "        outputs = self.forward(data1,data2)\n",
        "        \n",
        "        lossfn = nn.NLLLoss()\n",
        "        loss = lossfn(outputs, labels)\n",
        "        \n",
        "        pred = torch.exp(outputs)\n",
        "        #pred = output.data.max(1, keepdim=True)[1]\n",
        "        self.val_acc.update(pred, labels)\n",
        "        self.val_f1.update(pred, labels)\n",
        "\n",
        "        self.log(\"val_loss\", loss)\n",
        "        return {\"val_loss\": loss}\n",
        "    \n",
        "    def training_epoch_end(self, training_step_outputs):\n",
        "        # compute metrics\n",
        "        train_accuracy = self.train_acc.compute()\n",
        "        train_f1 = self.train_f1.compute()\n",
        "        # log metrics\n",
        "        self.log(\"epoch_train_accuracy\", train_accuracy)\n",
        "        self.log(\"epoch_train_f1\", train_f1)\n",
        "        # reset all metrics\n",
        "        self.train_acc.reset()\n",
        "        self.train_f1.reset()\n",
        "        print(f\"\\ntraining accuracy: {train_accuracy:.4}, \"\\\n",
        "        f\"f1: {train_f1:.4}\")\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        # outputs = list of dictionaries\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        val_accuracy = self.val_acc.compute()\n",
        "        val_f1 = self.val_f1.compute()\n",
        "        # log metrics\n",
        "        self.log(\"val_accuracy\", val_accuracy)\n",
        "        self.log(\"val_loss\", avg_loss)\n",
        "        self.log(\"val_f1\", val_f1)\n",
        "        # reset all metrics\n",
        "        self.val_acc.reset()\n",
        "        self.val_f1.reset()\n",
        "        print(f\"\\nvalidation accuracy: {val_accuracy:.4} \"\\\n",
        "        f\"f1: {val_f1:.4}\")\n",
        "        \n",
        "        tensorboard_logs = {'avg_val_loss': avg_loss}\n",
        "        # use key 'log'\n",
        "        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
        "    \n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:22:43.207791Z",
          "iopub.execute_input": "2022-05-22T10:22:43.208076Z",
          "iopub.status.idle": "2022-05-22T10:22:43.231936Z",
          "shell.execute_reply.started": "2022-05-22T10:22:43.208044Z",
          "shell.execute_reply": "2022-05-22T10:22:43.231162Z"
        },
        "trusted": true,
        "id": "DNM9ZJqxWPHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early Stopping"
      ],
      "metadata": {
        "id": "COjkQhv5WPHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# seeding for torch, numpy, stdlib random, including DataLoader workers!\n",
        "seed_everything(123, workers=True)\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    stopping_threshold=1e-5,\n",
        "    divergence_threshold=9.0,\n",
        "    check_finite=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:22:43.351166Z",
          "iopub.execute_input": "2022-05-22T10:22:43.351696Z",
          "iopub.status.idle": "2022-05-22T10:22:43.357046Z",
          "shell.execute_reply.started": "2022-05-22T10:22:43.351657Z",
          "shell.execute_reply": "2022-05-22T10:22:43.356025Z"
        },
        "trusted": true,
        "id": "JSobkXXNWPHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "TF67Kj5HWPHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LitLSTM(dimension=X_train_body.shape[2],hidd_dim=128,num_classes=3)\n",
        "trainer = Trainer(accelerator='gpu',devices=1,max_epochs=50,log_every_n_steps=8)\n",
        "trainer.fit(model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:22:43.446996Z",
          "iopub.execute_input": "2022-05-22T10:22:43.447376Z",
          "iopub.status.idle": "2022-05-22T10:25:14.684299Z",
          "shell.execute_reply.started": "2022-05-22T10:22:43.447342Z",
          "shell.execute_reply": "2022-05-22T10:25:14.677314Z"
        },
        "trusted": true,
        "id": "PmvWWMgpWPHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict On Review"
      ],
      "metadata": {
        "id": "LvN5MAlZWPHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predsentiment(rev):\n",
        "    print(rev)\n",
        "    print(\"==========================================================================================================================================\")\n",
        "    dftest = pd.DataFrame(columns=['review'])\n",
        "    dftest = dftest.append({'review': rev}, ignore_index=True)\n",
        "    T = dftest['review'].str.split(' \\n\\n---\\n\\n').str[0]\n",
        "    T = T.str.replace('-',' ').str.replace('[^\\w\\s]','').str.replace('\\n',' ').str.lower()\n",
        "    stop = stopwords.words('english')\n",
        "    T = T.apply(lambda x: ' '.join(x for x in x.split() if  not x.isdigit()))\n",
        "    T = T.apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n",
        "    x,_,_ = Sentence2Vec(T)\n",
        "    print(\"==========================================================================================================================================\")\n",
        "    X_test = torch.Tensor(x)\n",
        "    with torch.no_grad():\n",
        "        output = model(X_test)\n",
        "        output = torch.exp(output)\n",
        "        #print(output)\n",
        "        index = output.data.cpu().numpy().argmax()\n",
        "        result = list(np.around(output.data.cpu().numpy()*100,1))\n",
        "        print(\"PREDICTION PROBABILITY = \",result)\n",
        "        strn = \"POSITIVE\"\n",
        "        if(index==0): strn = \"NEGATIVE\"  \n",
        "        print(\"PREDICTED CLASS = \",strn)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-22T10:25:14.691057Z",
          "iopub.execute_input": "2022-05-22T10:25:14.706532Z",
          "iopub.status.idle": "2022-05-22T10:25:14.780364Z",
          "shell.execute_reply.started": "2022-05-22T10:25:14.706464Z",
          "shell.execute_reply": "2022-05-22T10:25:14.776265Z"
        },
        "trusted": true,
        "id": "-gMvk8YXWPHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Review from Movie RRR"
      ],
      "metadata": {
        "id": "HuvFWJOwWPHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Review from movie RACE-3"
      ],
      "metadata": {
        "id": "9LgVJNBfWPHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YXRWo-jCWPHb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}