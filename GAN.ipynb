{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAHbTD_mRG_3",
        "outputId": "8fc06970-a664-4789-829f-f5bedf40652a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _get_module_lock.<locals>.cb at 0x7fc156da71f0>\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 182, in cb\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 5, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/main.py\", line 9, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/main_parser.py\", line 8, in <module>\n",
            "    from pip._internal.cli import cmdoptions\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/cmdoptions.py\", line 23, in <module>\n",
            "    from pip._internal.cli.parser import ConfigOptionParser\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/parser.py\", line 12, in <module>\n",
            "    from pip._internal.configuration import Configuration, ConfigurationError\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/configuration.py\", line 20, in <module>\n",
            "    from pip._internal.exceptions import (\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/exceptions.py\", line 13, in <module>\n",
            "    from pip._vendor.requests.models import Request, Response\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/requests/__init__.py\", line 45, in <module>\n",
            "    from .exceptions import RequestsDependencyWarning\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/requests/exceptions.py\", line 11, in <module>\n",
            "    from .compat import JSONDecodeError as CompatJSONDecodeError\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/requests/compat.py\", line 11, in <module>\n",
            "    from pip._vendor import chardet\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/chardet/__init__.py\", line 19, in <module>\n",
            "    from .universaldetector import UniversalDetector\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/chardet/universaldetector.py\", line 48, in <module>\n",
            "    from .sbcsgroupprober import SBCSGroupProber\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/chardet/sbcsgroupprober.py\", line 31, in <module>\n",
            "    from .langbulgarianmodel import (ISO_8859_5_BULGARIAN_MODEL,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/chardet/langbulgarianmodel.py\", line 4, in <module>\n",
            "    from pip._vendor.chardet.sbcharsetprober import SingleByteCharSetModel\n",
            "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 844, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 939, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1037, in get_data\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNOfdSnORKQ4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "import os\n",
        "import cv2\n",
        "from torch.nn import functional as F\n",
        "from pathlib import Path\n",
        "import torchvision\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import timm\n",
        "from torch import nn\n",
        "import PIL\n",
        "import torchvision.models as models\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (AdamW,T5ForConditionalGeneration, AutoTokenizer as Tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9Gb3u_SRMG4",
        "outputId": "d802b6ee-72c0-4306-c634-99976054b670"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Global seed set to 42\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oEMhEsRWiG7",
        "outputId": "bd5d6679-a9fc-4ea6-ee31-b16440bbad1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "replace /content/val2014/COCO_val2014_000000324670.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "_URL = 'http://images.cocodataset.org/zips/val2014.zip'\n",
        "zip_dir = tf.keras.utils.get_file('/content/MSCOCOVAL2014.zip', origin=_URL, extract=False,archive_format='auto')\n",
        "fname = '/content/MSCOCOVAL2014.zip'\n",
        "!unzip -q $fname -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjdLlbHnA3Sr",
        "outputId": "1f1c8b0c-f788-49b6-d281-d89e8d5b7d5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "replace /content/v2_OpenEnded_mscoco_val2014_questions.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "_URL = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip'\n",
        "zip_dir = tf.keras.utils.get_file('/content/QUESVAL2014.zip', origin=_URL, extract=False,archive_format='auto')\n",
        "fname = '/content/QUESVAL2014.zip'\n",
        "!unzip -q $fname -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcOsBq-AA8d3",
        "outputId": "490b2e59-3e14-46dc-e02e-70698036a002"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "replace /content/v2_mscoco_val2014_annotations.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "_URL = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip'\n",
        "zip_dir = tf.keras.utils.get_file('/content/ANNOTVAL2014.zip', origin=_URL, extract=False,archive_format='auto')\n",
        "fname = '/content/ANNOTVAL2014.zip'\n",
        "!unzip -q $fname -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yscFtjMpzK2X"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join('/content/', 'v2_OpenEnded_mscoco_val2014_questions.json'), 'r') as f:\n",
        "    val_questions = json.load(f)['questions']\n",
        "with open(os.path.join('/content/', 'v2_mscoco_val2014_annotations.json'), 'r') as f:\n",
        "    val_answers = json.load(f)['annotations']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT4sIIxFES3T"
      },
      "outputs": [],
      "source": [
        "val_data = []\n",
        "for question, annotation in zip(val_questions, val_answers):\n",
        "    question_text = question['question']\n",
        "    image_id = annotation['image_id']\n",
        "    answer = annotation['answers'][0]['answer']\n",
        "    image_filename = 'COCO_val2014_{:012d}.jpg'.format(image_id)\n",
        "    image_path = os.path.join('/content/', 'val2014', image_filename)\n",
        "    val_data.append({'question': question_text, 'image_path': image_path, 'answer': answer})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Li9aZO2OFpqb"
      },
      "outputs": [],
      "source": [
        "# Convert the array of dictionaries to a DataFrame\n",
        "df = pd.DataFrame(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opxpMYEgI4GS"
      },
      "outputs": [],
      "source": [
        "def has_three_channels(image_path):\n",
        "    with PIL.Image.open(image_path) as img:\n",
        "        return img.mode == 'RGB'\n",
        "\n",
        "# Filter the DataFrame to keep only the images with 3 channels\n",
        "df = df[df['image_path'].apply(has_three_channels)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNmpqV-tG4B2"
      },
      "outputs": [],
      "source": [
        "def show_sample(idx=0):\n",
        "  print(\"Q : \",df.iloc[idx]['question'])\n",
        "  image = cv2.imread(df.iloc[idx]['image_path'])\n",
        "  image = cv2.resize(image, (224, 224))  \n",
        "  cv2_imshow(image)\n",
        "  print(\"A : \",df.iloc[idx]['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG_BUUm0G5OS"
      },
      "outputs": [],
      "source": [
        "show_sample(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ckKyeLKkB96"
      },
      "source": [
        "# **Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOB39GRskuAJ"
      },
      "outputs": [],
      "source": [
        "df = df[:5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EelSm8PMksf3"
      },
      "outputs": [],
      "source": [
        "train_df, val_df = train_test_split(df,test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHLjPJFCkGwF"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx]['image_path']\n",
        "        image = Image.open(img_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return dict(img = image)\n",
        "\n",
        "class ImageDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, train_df, val_df, batch_size=8, num_workers=4, transform=None):\n",
        "        super().__init__()\n",
        "        self.train_df = train_df\n",
        "        self.val_df = val_df\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.transform = transform\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = ImageDataset(self.train_df, transform=self.transform)\n",
        "        self.val_dataset = ImageDataset(self.val_df, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True, pin_memory=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return None # No test dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfPsWlRJlNgo"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ry2YrTkdlpzW"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim: int):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.fc = nn.Linear(latent_dim, 256 * 8 * 8)\n",
        "        self.conv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.fc(z)\n",
        "        x = x.view(-1, 256, 8, 8)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.tanh(self.conv3(x))\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
        "        self.fc = nn.Linear(256 * 8 * 8, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv2(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv3(x), 0.2)\n",
        "        x = x.view(-1, 256 * 8 * 8)\n",
        "        x = torch.sigmoid(self.fc(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFrOOjermEa1"
      },
      "outputs": [],
      "source": [
        "class GAN(pl.LightningModule):\n",
        "    def __init__(self, latent_dim: int = 64, lr: float = 0.0002, b1: float = 0.5, b2: float = 0.999):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.generator = Generator(latent_dim)\n",
        "        self.discriminator = Discriminator()\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.generator(z)\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        real_images = batch['img']\n",
        "        batch_size = real_images.size(0)\n",
        "        device = real_images.device\n",
        "        valid = torch.ones(batch_size, 1, device=device)\n",
        "        fake = torch.zeros(batch_size, 1, device=device)\n",
        "\n",
        "        if optimizer_idx == 0: # Train generator\n",
        "            z = torch.randn(batch_size, self.hparams.latent_dim, device=device)\n",
        "            gen_images = self.generator(z)\n",
        "            g_loss = F.binary_cross_entropy(self.discriminator(gen_images), valid)\n",
        "            self.log('g_loss', g_loss)\n",
        "            return g_loss\n",
        "        else: # Train discriminator\n",
        "            z = torch.randn(batch_size, self.hparams.latent_dim, device=device)\n",
        "            gen_images = self.generator(z).detach()\n",
        "\n",
        "            real_loss = F.binary_cross_entropy(self.discriminator(real_images), valid)\n",
        "            fake_loss = F.binary_cross_entropy(self.discriminator(gen_images), fake)\n",
        "            d_loss = (real_loss + fake_loss) / 2\n",
        "            self.log('d_loss', d_loss)\n",
        "            return d_loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        real_images = batch['img']\n",
        "        batch_size = real_images.size(0)\n",
        "        device = real_images.device\n",
        "        valid = torch.ones(batch_size, 1, device=device)\n",
        "        fake = torch.zeros(batch_size, 1, device=device)\n",
        "\n",
        "        z = torch.randn(batch_size, self.hparams.latent_dim, device=device)\n",
        "        gen_images = self.generator(z).detach()\n",
        "\n",
        "        real_loss = F.binary_cross_entropy(self.discriminator(real_images), valid)\n",
        "        fake_loss = F.binary_cross_entropy(self.discriminator(gen_images), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        self.log('val_loss', d_loss, prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.hparams.lr\n",
        "        betas = (self.hparams.b1, self.hparams.b2)\n",
        "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=betas)\n",
        "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=betas)\n",
        "        return [opt_g, opt_d], []\n",
        "\n",
        "    def train_epoch_end(self):\n",
        "        z = torch.randn(8, self.hparams.latent_dim, device=self.device)\n",
        "        sample_imgs = self.generator(z)\n",
        "        grid = torchvision.utils.make_grid(sample_imgs, nrow=4, normalize=True)\n",
        "        self.logger.experiment.add_image('generated_images', grid, global_step=self.current_epoch)\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        if len(outputs) > 0:\n",
        "          avg_val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "          self.log('val_loss', avg_val_loss, prog_bar=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdnfJeNnlWs-"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "N_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONaS4ahslM16"
      },
      "outputs": [],
      "source": [
        "data_module = ImageDataModule(train_df, val_df, batch_size=BATCH_SIZE, \n",
        "                              num_workers=4, transform=transform)\n",
        "data_module.setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k8xjBqxu4ci"
      },
      "outputs": [],
      "source": [
        "model = GAN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOsQbdPOu49R"
      },
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath = 'checkpoints',\n",
        "    filename = 'best_cp',\n",
        "    save_top_k = 1,\n",
        "    verbose = True,\n",
        "    monitor = 'val_loss',\n",
        "    mode = 'min')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H3UPQYxvC0f"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    callbacks=[checkpoint_callback],\n",
        "    max_epochs = N_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leMcphxIvIBf"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model,data_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3KSmn_v5trd"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Define the transformation to apply on the input image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load the image from file\n",
        "image = Image.open(\"/content/val2014/COCO_val2014_000000000042.jpg\")\n",
        "cv2_imshow(image)\n",
        "\n",
        "# Apply the transformation\n",
        "image = transform(image)\n",
        "\n",
        "# Load the saved checkpoint\n",
        "checkpoint_path = \"/content/checkpoints/best_cp.ckpt\"\n",
        "model = GAN.load_from_checkpoint(checkpoint_path)\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Generate the image\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(1, model.hparams.latent_dim, device=model.device)\n",
        "    gen_image = model.generator(z)\n",
        "    gen_image = (gen_image + 1) / 2  # Convert the output to a valid image range (0, 1)\n",
        "\n",
        "# Display the generated image\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(gen_image[0].permute(1, 2, 0).cpu())\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}