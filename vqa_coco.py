# -*- coding: utf-8 -*-
"""VQA-COCO.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lyV2OuIQRtIEyv7GR8nTux6P5a6-K70F
"""

!pip install --quiet transformers
!pip install --quiet pytorch-lightning
!pip install --quiet tokenizers
!pip install --quiet timm

import tensorflow as tf
import json
import os
import cv2
from google.colab.patches import cv2_imshow
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
from pathlib import Path
import torchvision.transforms as transforms
from sklearn.model_selection import train_test_split
import timm
from torch import nn
import PIL
import torchvision.models as models

import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from sklearn.model_selection import train_test_split
from transformers import (AdamW,T5ForConditionalGeneration, AutoTokenizer as Tokenizer)

"""# **Pre-Process the Data**"""

_URL = 'http://images.cocodataset.org/zips/val2014.zip'
zip_dir = tf.keras.utils.get_file('/content/MSCOCOVAL2014.zip', origin=_URL, extract=False,archive_format='auto')
fname = '/content/MSCOCOVAL2014.zip'
!unzip -q $fname -d /content/

_URL = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip'
zip_dir = tf.keras.utils.get_file('/content/QUESVAL2014.zip', origin=_URL, extract=False,archive_format='auto')
fname = '/content/QUESVAL2014.zip'
!unzip -q $fname -d /content/

_URL = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip'
zip_dir = tf.keras.utils.get_file('/content/ANNOTVAL2014.zip', origin=_URL, extract=False,archive_format='auto')
fname = '/content/ANNOTVAL2014.zip'
!unzip -q $fname -d /content/

valtxt = 'https://raw.githubusercontent.com/jponttuset/mcg/master/pre-trained/datasets/COCO/gt_sets/val2014.txt'
zip_dir = tf.keras.utils.get_file('/content/valtxt.txt', origin=valtxt, extract=False,archive_format='auto')

with open(os.path.join('/content/', 'v2_OpenEnded_mscoco_val2014_questions.json'), 'r') as f:
    val_questions = json.load(f)['questions']
with open(os.path.join('/content/', 'v2_mscoco_val2014_annotations.json'), 'r') as f:
    val_answers = json.load(f)['annotations']

val_data = []
for question, annotation in zip(val_questions, val_answers):
    question_text = question['question']
    image_id = annotation['image_id']
    answer = annotation['answers'][0]['answer']
    image_filename = 'COCO_val2014_{:012d}.jpg'.format(image_id)
    image_path = os.path.join('/content/', 'val2014', image_filename)
    val_data.append({'question': question_text, 'image_path': image_path, 'answer': answer})

# Convert the array of dictionaries to a DataFrame
df = pd.DataFrame(val_data)

df

def show_sample(idx=0):
  print("Q : ",df.iloc[idx]['question'])
  image = cv2.imread(df.iloc[idx]['image_path'])
  image = cv2.resize(image, (224, 224))  
  cv2_imshow(image)
  print("A : ",df.iloc[idx]['answer'])

show_sample(10)

"""# **DataLoader**"""

max_length = len(df['answer'].max())
print(max_length)

max_length = len(df['question'].max())
print(max_length)

MODEL_NAME = 't5-small'

tokenizer = Tokenizer.from_pretrained(MODEL_NAME)

train_df, val_df = train_test_split(df,test_size=0.1)

transform = transforms.Compose([
    transforms.Resize((299, 299)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])

class VQADataset(Dataset):
    def __init__(self, df, transform,tokenizer):
        self.df = df
        self.transform = transform
        self.tokenizer = tokenizer
        self.img_model = timm.create_model('xception', pretrained=True)
        self.img_model.aux_logits=False
        self.img_model.train(False)


        # Freeze training for all layers
        for param in self.img_model.parameters():
            param.requires_grad = False
        
        self.img_model.fc = torch.nn.Sequential(torch.nn.Linear(self.img_model.fc.in_features, 256),
                          torch.nn.Dropout(0.5),
                          torch.nn.ReLU(inplace=True),
                          torch.nn.BatchNorm1d(256),
                          torch.nn.Linear(256, 64))
        
        self.img_model.eval()

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        row = self.df.iloc[index]
        question = row['question']
        image_path = row['image_path']
        answer = row['answer']

        answer = self.tokenizer(answer, padding='max_length',truncation = True,return_attention_mask = True,add_special_tokens = True, max_length=64, return_tensors='pt')

        labels = answer["input_ids"]
        labels[labels == 0] = -100
        
        # load the image and apply transform
        img = PIL.Image.open(image_path)

        img_array = np.array(img)
        if img_array.shape[0] == 1:
            img_array = np.repeat(img_array, 3, axis=0)

        # Convert the NumPy array back to a PIL image and apply the transforms
        img = PIL.Image.fromarray(img_array)
        
        img = transform(img)


        img = img.unsqueeze(0)

        # Generate the image embedding
        with torch.no_grad():
            features = self.img_model(img)

        img = img.squeeze()
        
        img = ','.join(str(x) for x in img)
        quesimg = f"question: {question} context: {img}"

        quesimg = self.tokenizer(quesimg, padding='max_length',truncation = True,return_attention_mask = True,add_special_tokens = True, max_length=140, return_tensors='pt')

        
        return dict(
        input_ids = quesimg['input_ids'].flatten(),
        attention_mask = quesimg['attention_mask'].flatten(),
        labels = labels.flatten())

class VQADataModule(pl.LightningDataModule):
  def __init__(self,train_df : pd.DataFrame,test_df : pd.DataFrame,transform : transform,tokenizer : tokenizer,batch_size : int = 8):
    super().__init__()
    self.batch_size = batch_size
    self.train_df = train_df
    self.test_df = test_df
    self.tokenizer = tokenizer
    self.transform = transform


  def setup(self,stage=None):
    self.train_dataset = VQADataset(self.train_df,self.transform,self.tokenizer)
    self.test_dataset = VQADataset(self.test_df,self.transform,self.tokenizer)

  def train_dataloader(self):
    return DataLoader(self.train_dataset,batch_size = self.batch_size,shuffle=True,num_workers=4)

  def val_dataloader(self):
    return DataLoader(self.test_dataset,batch_size = self.batch_size,num_workers=4)

  def test_dataloader(self):
    return DataLoader(self.test_dataset,batch_size = self.batch_size,num_workers=4)

BATCH_SIZE = 8
N_EPOCHS = 5

data_module = VQADataModule(train_df,val_df,transform,tokenizer,batch_size = BATCH_SIZE)
data_module.setup()

"""# **Model Architecture**"""

class VQAModel(pl.LightningModule):
  def __init__(self):
    super().__init__()

    self.text_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME,return_dict=True)
    self.lossfn = nn.CrossEntropyLoss()
                  

  def forward(self,input_ids,attention_mask,labels=None):
    output = self.text_model(input_ids = input_ids,attention_mask = attention_mask,labels = labels)
    return output.loss, output.logits

  def training_step(self,batch,batch_idx):
    input_ids = batch['input_ids']
    attention_mask = batch['attention_mask']
    labels = batch['labels']
    loss, outputs = self(input_ids,attention_mask,labels)
    self.log("train_loss",loss,prog_bar=True,logger=True)
    return loss

  def validation_step(self,batch,batch_idx):
    input_ids = batch['input_ids']
    attention_mask = batch['attention_mask']
    labels = batch['labels']
    loss, outputs = self(input_ids,attention_mask,labels)
    self.log("val_loss",loss,prog_bar=True,logger=True)
    return loss

  def test_step(self,batch,batch_idx):
    input_ids = batch['input_ids']
    attention_mask = batch['attention_mask']
    labels = batch['labels']
    loss, outputs = self(input_ids,attention_mask,labels)
    self.log("test_loss",loss,prog_bar=True,logger=True)
    return loss

  def configure_optimizers(self):
    return AdamW(self.parameters(),lr = 0.0001)

"""# **Training**"""

model = VQAModel()

checkpoint_callback = ModelCheckpoint(
    dirpath = 'checkpoints',
    filename = 'best_cp',
    save_top_k = 1,
    verbose = True,
    monitor = 'val_loss',
    mode = 'min')

trainer = pl.Trainer(gpus=1,
    callbacks=[checkpoint_callback],
    max_epochs = 5)

trainer.fit(model,data_module)