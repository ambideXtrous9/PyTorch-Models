# -*- coding: utf-8 -*-
"""VQA-COCO-Concat-T5Transformer-EfficientNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ldY8GN_aZH0FKEieTFZDgP1ft5e4JKzF
"""

!pip install --quiet transformers
!pip install --quiet pytorch-lightning
!pip install --quiet tokenizers
!pip install --quiet timm

import tensorflow as tf
import json
import os
import cv2
from google.colab.patches import cv2_imshow
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
from pathlib import Path
import torchvision.transforms as transforms
from sklearn.model_selection import train_test_split
import timm
from torch import nn
import PIL
import torchvision.models as models

import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from sklearn.model_selection import train_test_split
from transformers import (AdamW,T5ForConditionalGeneration, AutoTokenizer as Tokenizer)

pl.seed_everything(42)

"""# **Pre-Process the Data**"""

_URL = 'http://images.cocodataset.org/zips/val2014.zip'
zip_dir = tf.keras.utils.get_file('/content/MSCOCOVAL2014.zip', origin=_URL, extract=False,archive_format='auto')
fname = '/content/MSCOCOVAL2014.zip'
!unzip -q $fname -d /content/

_URL = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip'
zip_dir = tf.keras.utils.get_file('/content/QUESVAL2014.zip', origin=_URL, extract=False,archive_format='auto')
fname = '/content/QUESVAL2014.zip'
!unzip -q $fname -d /content/

_URL = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip'
zip_dir = tf.keras.utils.get_file('/content/ANNOTVAL2014.zip', origin=_URL, extract=False,archive_format='auto')
fname = '/content/ANNOTVAL2014.zip'
!unzip -q $fname -d /content/

valtxt = 'https://raw.githubusercontent.com/jponttuset/mcg/master/pre-trained/datasets/COCO/gt_sets/val2014.txt'
zip_dir = tf.keras.utils.get_file('/content/valtxt.txt', origin=valtxt, extract=False,archive_format='auto')

with open(os.path.join('/content/', 'v2_OpenEnded_mscoco_val2014_questions.json'), 'r') as f:
    val_questions = json.load(f)['questions']
with open(os.path.join('/content/', 'v2_mscoco_val2014_annotations.json'), 'r') as f:
    val_answers = json.load(f)['annotations']

val_data = []
for question, annotation in zip(val_questions, val_answers):
    question_text = question['question']
    image_id = annotation['image_id']
    answer = annotation['answers'][0]['answer']
    image_filename = 'COCO_val2014_{:012d}.jpg'.format(image_id)
    image_path = os.path.join('/content/', 'val2014', image_filename)
    val_data.append({'question': question_text, 'image_path': image_path, 'answer': answer})

# Convert the array of dictionaries to a DataFrame
df = pd.DataFrame(val_data)

df

def show_sample(idx=0):
  print("Q : ",df.iloc[idx]['question'])
  image = cv2.imread(df.iloc[idx]['image_path'])
  image = cv2.resize(image, (224, 224))  
  cv2_imshow(image)
  print("A : ",df.iloc[idx]['answer'])

show_sample(100)



"""# **DataLoader**"""

def has_three_channels(image_path):
    with PIL.Image.open(image_path) as img:
        return img.mode == 'RGB'

# Filter the DataFrame to keep only the images with 3 channels
df = df[df['image_path'].apply(has_three_channels)]

max_length = len(df['answer'].max())
print(max_length)

max_length = len(df['question'].max())
print(max_length)

df = df[:5000]

MODEL_NAME = 't5-base'

tokenizer = Tokenizer.from_pretrained(MODEL_NAME)

train_df, val_df = train_test_split(df,test_size=0.1)
val_df, test_df = train_test_split(val_df,test_size=0.5)

train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])

val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()])

class VQADataset(Dataset):
    def __init__(self, df, transform,tokenizer):
        self.df = df
        self.transform = transform
        self.tokenizer = tokenizer
        self.img_model = timm.create_model('efficientnet_b0', pretrained=True)
        self.img_model.aux_logits=False

        # Freeze training for all layers
        for param in self.img_model.parameters():
            param.requires_grad = False
        
        self.img_model.classifier = torch.nn.Sequential(
                          torch.nn.Linear(self.img_model.classifier.in_features, 256),
                          torch.nn.Dropout(0.5),
                          torch.nn.ReLU(inplace=True),
                          torch.nn.BatchNorm1d(256),
                          torch.nn.Linear(256, 64))
        
        self.img_model.eval()

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        row = self.df.iloc[index]
        question = row['question']
        image_path = row['image_path']
        answer = row['answer']

        answer = self.tokenizer(answer, padding='max_length',truncation = True,
                                return_attention_mask = True,add_special_tokens = True, 
                                max_length=64, return_tensors='pt')

        labels = answer["input_ids"]
        labels[labels == 0] = -100
        
        # load the image and apply transform
        img = PIL.Image.open(image_path)
        img = self.transform(img)
        img = img.unsqueeze(0)

        # Generate the image embedding
        with torch.no_grad():
            img = self.img_model(img)

        img = img.squeeze()
        
        img = ' '.join(str(x) for x in img)
        quesimg = f"question: {question} context: {img}"

        quesimg = self.tokenizer(quesimg, padding='max_length',truncation = True,
                                 return_attention_mask = True,add_special_tokens = True, 
                                 max_length=200, return_tensors='pt')

        
        return dict(
        input_ids = quesimg['input_ids'].flatten(),
        attention_mask = quesimg['attention_mask'].flatten(),
        labels = labels.flatten())

class VQADataModule(pl.LightningDataModule):
  def __init__(self,train_df : pd.DataFrame,val_df : pd.DataFrame,test_df : pd.DataFrame,
               train_transform : train_transform,val_transform : val_transform,tokenizer : tokenizer,batch_size : int = 8):
    super().__init__()
    self.batch_size = batch_size
    self.train_df = train_df
    self.test_df = test_df
    self.val_df = val_df
    self.tokenizer = tokenizer
    self.train_transform = train_transform
    self.val_transform = val_transform


  def setup(self,stage=None):
    self.train_dataset = VQADataset(self.train_df,self.train_transform,self.tokenizer)
    self.val_dataset = VQADataset(self.val_df,self.val_transform,self.tokenizer)
    self.test_dataset = VQADataset(self.test_df,self.val_transform,self.tokenizer)

  def train_dataloader(self):
    return DataLoader(self.train_dataset,batch_size = self.batch_size,shuffle=True,num_workers=4)

  def val_dataloader(self):
    return DataLoader(self.val_dataset,batch_size = self.batch_size,num_workers=4)

  def test_dataloader(self):
    return DataLoader(self.test_dataset,batch_size = self.batch_size,num_workers=4)

BATCH_SIZE = 12
N_EPOCHS = 10

data_module = VQADataModule(train_df,val_df,test_df,train_transform,
                            val_transform,tokenizer,batch_size = BATCH_SIZE)
data_module.setup()

"""# **Model Architecture**"""

class VQAModel(pl.LightningModule):
  def __init__(self):
    super().__init__()

    self.text_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME,return_dict=True)
    self.lossfn = nn.CrossEntropyLoss()
                  

  def forward(self,input_ids,attention_mask,labels=None):
    output = self.text_model(input_ids = input_ids,attention_mask = attention_mask,labels = labels)
    return output.loss, output.logits

  def training_step(self,batch,batch_idx):
    input_ids = batch['input_ids']
    attention_mask = batch['attention_mask']
    labels = batch['labels']
    loss, outputs = self(input_ids,attention_mask,labels)
    self.log("train_loss",loss,prog_bar=True,logger=True)
    return loss

  def validation_step(self,batch,batch_idx):
    input_ids = batch['input_ids']
    attention_mask = batch['attention_mask']
    labels = batch['labels']
    loss, outputs = self(input_ids,attention_mask,labels)
    self.log("val_loss",loss,prog_bar=True,logger=True)
    return loss

  def test_step(self,batch,batch_idx):
    input_ids = batch['input_ids']
    attention_mask = batch['attention_mask']
    labels = batch['labels']
    loss, outputs = self(input_ids,attention_mask,labels)
    self.log("test_loss",loss,prog_bar=True,logger=True)
    return loss

  def configure_optimizers(self):
    return AdamW(self.parameters(),lr = 0.0001)

"""# **Training**"""

model = VQAModel()

checkpoint_callback = ModelCheckpoint(
    dirpath = 'checkpoints',
    filename = 'best_cp',
    save_top_k = 1,
    verbose = True,
    monitor = 'val_loss',
    mode = 'min')

trainer = pl.Trainer(gpus=1,
    callbacks=[checkpoint_callback],
    max_epochs = N_EPOCHS)

trainer.fit(model,data_module)

"""# **Test on Image and Text**"""

cppath = '/content/checkpoints/best_cp.ckpt'
trained_model = VQAModel.load_from_checkpoint(cppath)
trained_model.freeze()
trainer.test(trained_model, data_module)

def generate_ans(idx=0,dframe=test_df):
    img_model = timm.create_model('efficientnet_b0', pretrained=True)
    img_model.aux_logits=False

    # Freeze training for all layers
    for param in img_model.parameters():
        param.requires_grad = False
    
    img_model.classifier = torch.nn.Sequential(
                      torch.nn.Linear(img_model.classifier.in_features, 256),
                      torch.nn.Dropout(0.5),
                      torch.nn.ReLU(inplace=True),
                      torch.nn.BatchNorm1d(256),
                      torch.nn.Linear(256, 64))
    
    img_model.eval()

    row = dframe.iloc[idx]
    question = row['question']
    image_path = row['image_path']
    answer = row['answer']

    print("QUESTION : ",question)

    image = cv2.imread(image_path)
    image = cv2.resize(image, (224, 224))  
    cv2_imshow(image)

    print("ANSWER : ",answer)

    transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()])
  
    # load the image and apply transform
    img = PIL.Image.open(image_path)
    img.show()
    img = transform(img)
    img = img.unsqueeze(0)

    # Generate the image embedding
    with torch.no_grad():
        img = img_model(img)

    img = img.squeeze()
    
    img = ','.join(str(x) for x in img)
    quesimg = f"question: {question} context: {img}"

    quesimg = tokenizer(quesimg, padding='max_length',truncation = True,
                              return_attention_mask = True,add_special_tokens = True, 
                              max_length=200, return_tensors='pt')
    
    input_ids = quesimg['input_ids']
    attention_mask = quesimg['attention_mask']

    generated_ids = trained_model.text_model.generate(
        input_ids = input_ids,
        attention_mask = attention_mask,
        num_beams = 1,
        max_length = 64,
        repetition_penalty = 2.5,
        length_penalty = 1.0,
        early_stopping = True,
        use_cache = True)
    
    preds = [
        tokenizer.decode(generated_id,skip_special_tokens=True,clean_up_tokenization_spaces=True)
        for generated_id in generated_ids]
    
    return " ".join(preds)

ans = generate_ans(55,test_df)
print("PREDICTED ANSWER : ",ans)

ans = generate_ans(1,test_df)
print("PREDICTED ANSWER : ",ans)

ans = generate_ans(65,test_df)
print("PREDICTED ANSWER : ",ans)

ans = generate_ans(2,test_df)
print("PREDICTED ANSWER : ",ans)

